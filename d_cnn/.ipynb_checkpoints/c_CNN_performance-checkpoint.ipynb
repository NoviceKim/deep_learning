{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02c3a63-ab4e-47eb-84a0-82424fe17bb1",
   "metadata": {},
   "source": [
    "### CNN Performance\n",
    "- CNN 모델을 제작할 때, 다양한 기법을 통해 성능 개선 및 과적합 해소가 가능하다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Weight Initialization (가중치 초기화)\n",
    "- 처음 가중치를 어떻게 줄 것인지 정하는 방법이며, 처음 가중치를 어떻게 설정하느냐에 따라 모델의 성능이 크게 달라진다.\n",
    "\n",
    "> 1. 사비에르 글로로트 초기화\n",
    "> - 고정된 표준편차를 사용하지 않고, 이전 층의 노드 수에 맞게 현재 층의 가중치를 초기화한다.\n",
    "> - 층마다 노드 개수를 다르게 설정하더라도, 이에 맞게 가중치가 초기화되기 때문에  \n",
    "고정된 표준편차를 사용하는 것보다 이상치에 민감하지 않다.\n",
    "> - 활성화 함수가 ReLU일 경우, 층을 통과할수록 활성화 값이 고르지 못하게 되는 문제가 있기 때문에 출력층에서만 사용한다.\n",
    "> \n",
    "> <div style='display: flex;' style='margin-left: 50px;'>\n",
    "    <div>\n",
    "        <img src='./images/xavier01.png'>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='./images/xavier02.png' style='margin-left: 50px;'>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "> 2. 카이밍 히 초기화\n",
    "> - 고정된 표준편차를 사용하지 않고, 이전 층의 노드 수에 맞게 현재 층의 가중치를 초기화한다.\n",
    "> - 층마다 노드 개수를 다르게 설정하더라도, 이에 맞게 가중치가 초기화되기 때문에  \n",
    "고정된 표준편차를 사용하는 것보다 이상치에 민감하지 않다.\n",
    "> - 활성화 함수가 ReLU일 때 추천하는 초기화 방식으로서, 층이 깊어지더라도 모든 활성화 값이 고르게 분포된다.\n",
    "> <img src='./images/he.png' style='margin-left: 50px;'>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Batch Normalization (배치 정규화)\n",
    "- 입력 데이터 간 값의 차이가 발생하면, 그에 따라 가중치의 비중도 달라지기 때문에 층을 통과할수록 편차가 심해진다.\n",
    "  이를 내부 공변량 이동(Internal Convariant Shift)이라고 한다.\n",
    "- 가중치의 값의 비중이 달라지면, 특정 가중치에 중점을 두면서 경사 하강법이 진행되기 때문에\n",
    "  모든 입력값을 표준 정규화하여 최적의 파라미터를 보다 빠르게 학습할 수 있도록 해야한다.\n",
    "- 가중치를 표준화할 때 민감도를 감소시키고, 학습 속도를 증가시키며, 모델을 일반화하기 위해 사용한다.\n",
    "\n",
    "<div style='display: flex;' width='90%'>\n",
    "    <div>\n",
    "        <img src='./images/BN01.png' width='900px' style='margin-top: 20px;'>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='./images/BN02.png' width='900px'>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "- BN을 활성화 함수앞에 적용하면, Weight(가중치) 값은 평균이 0, 분산이 1인 상태로 정규분포가 된다.\n",
    "- ReLU가 activation 파라미터로 적용되면, 음수에 해당하는 부분(절반 정도)이 0이 된다.\n",
    "- 이러한 문제를 해결하기 위해 γ(감마)와 β(베타)를 활용해서 음수 부분이 모두 0이 되는 것을 막아준다.\n",
    "\n",
    "<div style='display: flex;' width='70%'>\n",
    "    <div>\n",
    "        <img src='./images/BN03.png' width='1000px' style='margin-top: 20px;'>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='./images/BN04.png' width='800px'>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
