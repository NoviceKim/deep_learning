{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b6871c-6c82-48e1-8eb3-d929acd9e8ad",
   "metadata": {},
   "source": [
    "### Callback API\n",
    "- 모델이 학습 중에 충돌이 발생하거나 네트워크가 끊기면 모든 훈련 시간이 낭비될 수 있고,  \n",
    "  과적합을 방지하기 위해 훈련을 조기 종료해야 할 수도 있다.\n",
    "- 모델이 학습을 시작하면 학습이 완료될 때까지 어떤 제어도 하지 못하게 되고,  \n",
    "  신경망 훈련을 완료하는 데에는 몇 시간에서 최대 며칠이 걸릴 수도 있기 때문에, 모델을 모니터링 및 제어하는 기능이 필요하다.\n",
    "- 훈련 시(fit()) Callback API를 등록시키면 반복 내에서 특정 이벤트가 발생할 때마다 등록된 callback이 호출되어 수행된다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weight_only=False, mode='auto')**\n",
    "\n",
    "- 특정 조건에 따라 모델 또는 가중치를 파일로 저장한다.\n",
    "- filepath: \"weights.{epochs:03d}-{val_loss:.4f}-{acc:.4f}.weights.hdf5\"와 같이 모델의 체크포인트를 저장한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- save_best_only: 가장 좋은 성능을 보인 모델을 저장할지에 대한 여부\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 적합한 것을 선택한다.  \n",
    "  monitor의 성능 지표가 감소해야 좋은 경우는 min, 증가해야 좋은 경우는 max,  \n",
    "  monitor의 성능 지표명으로부터 자동으로 유추하고 싶다면 auto를 기재한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_lr=0)**\n",
    "\n",
    "- 특정 반복 동안 성능이 개선되지 않을 때, 학습률을 동적으로 감소시킨다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- factor: 학습률을 감소시킬 비율, 새로운 학습률 = 기존 학습률 * factor\n",
    "- patience: 학습률을 줄이기 전 monitor 할 반복 횟수\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 적합한 것을 선택한다.  \n",
    "  monitor의 성능 지표가 감소해야 좋은 경우는 min, 증가해야 좋은 경우는 max,  \n",
    "  monitor의 성능 지표명으로부터 자동으로 유추하고 싶다면 auto를 기재한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')**\n",
    "\n",
    "- 특정 반복동안 성능이 개선되지 않으면 학습을 조기 종료한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- patience: Early Stopping을 적용하기 전 monitor할 반복 횟수\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 적합한 것을 선택한다.  \n",
    "  monitor의 성능 지표가 감소해야 좋은 경우는 min, 증가해야 좋은 경우는 max,  \n",
    "  monitor의 성능 지표명으로부터 자동으로 유추하고 싶다면 auto를 기재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad33d09d-c472-4d47-bb91-ca2bf09202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "# 모델 생성 함수 선언\n",
    "def create_model():\n",
    "    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n",
    "\n",
    "    x = Flatten()(input_tensor)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd691fe-5905-49c2-bc5f-277488c87ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# images MinMaxScaling 함수 선언\n",
    "def get_preprocessed_data(images, targets):\n",
    "    images = np.array(images / 255.0, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# targets 원핫 인코딩 함수 선언 (+ images MinMaxScaling)\n",
    "def get_preprocessed_ohe(images, targets):\n",
    "    images, targets = get_preprocessed_data(images, targets)\n",
    "    oh_targets = to_categorical(targets)\n",
    "\n",
    "    return images, oh_targets\n",
    "\n",
    "# train 데이터에서 validation 데이터 분리하는 함수 선언 (+ images MinMaxScaling, targets 원핫 인코딩)\n",
    "def get_train_valid_test(train_images, train_targets, test_images, test_targets, validation_size=0.2, random_state=124):\n",
    "    train_images, train_oh_targets = get_preprocessed_ohe(train_images, train_targets)\n",
    "    test_images, test_oh_targets = get_preprocessed_ohe(test_images, test_targets)\n",
    "\n",
    "    train_images, validation_images, train_oh_targets, validation_oh_targets = \\\n",
    "    train_test_split(train_images, train_oh_targets, stratify=train_oh_targets, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    return (train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0032a3c-7de7-4b0a-9c18-96cc8a463863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28) (48000, 10)\n",
      "(12000, 28, 28) (12000, 10)\n",
      "(10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# fashion 데이터 세트 불러오기\n",
    "(train_images, train_targets), (test_images, test_targets) = fashion_mnist.load_data()\n",
    "\n",
    "# 위에서 선언한 함수로 스케일링, 원핫 인코딩, validation 데이터 분리\n",
    "(train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets) = \\\n",
    "                                            get_train_valid_test(train_images, train_targets, test_images, test_targets)\n",
    "\n",
    "# train, validation, test 데이터 shape 출력\n",
    "print(train_images.shape, train_oh_targets.shape)\n",
    "print(validation_images.shape, validation_oh_targets.shape)\n",
    "print(test_images.shape, test_oh_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0faeac14-9128-49ff-a3d5-a5b354d323d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: ED2A-EA84\n",
      "\n",
      " C:\\KDT_0900_KGH\\ai\\deep_learning\\c_tensorflow\\callback_files 디렉터리\n",
      "\n",
      "2024-05-27  오후 01:25    <DIR>          .\n",
      "2024-05-28  오전 09:52    <DIR>          ..\n",
      "               0개 파일                   0 바이트\n",
      "               2개 디렉터리  101,143,785,472 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "# callback_files 경로 표시\n",
    "!dir callback_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887940bf-b5a8-4909-986c-19d8d287e70d",
   "metadata": {},
   "source": [
    "#### ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e65cf5d-4131-4224-950c-9c1c8031e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m735/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.7332 - loss: 0.7715"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Failed to format this callback filepath: \"./callback_files/weights.{epochs:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\". Reason: \\'epochs\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     10\u001b[0m mcp_cb \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./callback_files/weights.\u001b[39m\u001b[38;5;132;01m{epochs:03d}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_loss:.4f}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{acc:.4f}\u001b[39;00m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 모델 훈련과 동시에 train, validation 데이터에 대한 정확도와 loss 이력 저장\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mtrain_images, \n\u001b[0;32m     22\u001b[0m                     y\u001b[38;5;241m=\u001b[39mtrain_oh_targets, \n\u001b[0;32m     23\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m     24\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[0;32m     25\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(validation_images, validation_oh_targets), callbacks\u001b[38;5;241m=\u001b[39m[mcp_cb])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:323\u001b[0m, in \u001b[0;36mModelCheckpoint._get_file_path\u001b[1;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[0;32m    319\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    320\u001b[0m             epoch\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogs\n\u001b[0;32m    321\u001b[0m         )\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to format this callback filepath: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Failed to format this callback filepath: \"./callback_files/weights.{epochs:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\". Reason: \\'epochs\\''"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 모델 컴파일링\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "# callback (ModelCheckpoint) 선언 - 가중치(weight)만 저장\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath='./callback_files/weights.{epochs:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5',\n",
    "    monitor='val_loss',\n",
    "\n",
    "    # save_best_only=True: 모든 epoch의 파일을 저장하지 않고, 성능이 좋다고 판단했을 경우에만 저장\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 모델 훈련과 동시에 train, validation 데이터에 대한 정확도와 loss 이력 저장\n",
    "history = model.fit(x=train_images, \n",
    "                    y=train_oh_targets, \n",
    "                    batch_size=64, \n",
    "                    epochs=20, \n",
    "                    validation_data=(validation_images, validation_oh_targets), callbacks=[mcp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9602067f-62ad-4a35-a39c-fa981ca21462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: ED2A-EA84\n",
      "\n",
      " C:\\KDT_0900_KGH\\ai\\deep_learning\\c_tensorflow\\callback_files 디렉터리\n",
      "\n",
      "2024-05-27  오후 01:25    <DIR>          .\n",
      "2024-05-28  오전 09:52    <DIR>          ..\n",
      "               0개 파일                   0 바이트\n",
      "               2개 디렉터리  101,143,785,472 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "# callback_files 경로 표시\n",
    "!dir callback_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e682e22-2d06-4bba-8057-49f05c404a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a81b06-ff74-4c02-a011-6fa3a358120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "model.load_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e580-0d54-40f2-ad80-b2b8cc031a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643283d-cc12-43f5-9fa1-7a4e6cf66c06",
   "metadata": {},
   "source": [
    "#### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1aa395-4028-4e0a-b2dd-b92b4c1ac12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7408 - loss: 0.7531 - val_acc: 0.8527 - val_loss: 0.4087 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8583 - loss: 0.3960 - val_acc: 0.8502 - val_loss: 0.4005 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8711 - loss: 0.3604 - val_acc: 0.8644 - val_loss: 0.3624 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8776 - loss: 0.3301 - val_acc: 0.8734 - val_loss: 0.3537 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8876 - loss: 0.3097 - val_acc: 0.8673 - val_loss: 0.3567 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8900 - loss: 0.2969 - val_acc: 0.8771 - val_loss: 0.3310 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8955 - loss: 0.2814 - val_acc: 0.8814 - val_loss: 0.3228 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8991 - loss: 0.2704 - val_acc: 0.8753 - val_loss: 0.3407 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9016 - loss: 0.2633 - val_acc: 0.8859 - val_loss: 0.3234 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9170 - loss: 0.2245 - val_acc: 0.8911 - val_loss: 0.3016 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9187 - loss: 0.2195 - val_acc: 0.8910 - val_loss: 0.2998 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9211 - loss: 0.2134 - val_acc: 0.8918 - val_loss: 0.3010 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9248 - loss: 0.2073 - val_acc: 0.8926 - val_loss: 0.3027 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9231 - loss: 0.2039 - val_acc: 0.8942 - val_loss: 0.2988 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9249 - loss: 0.2041 - val_acc: 0.8932 - val_loss: 0.2987 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9262 - loss: 0.2030 - val_acc: 0.8938 - val_loss: 0.2985 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9263 - loss: 0.2010 - val_acc: 0.8929 - val_loss: 0.2984 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9243 - loss: 0.2026 - val_acc: 0.8931 - val_loss: 0.2986 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9276 - loss: 0.1972 - val_acc: 0.8929 - val_loss: 0.2985 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9251 - loss: 0.2015 - val_acc: 0.8932 - val_loss: 0.2985 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# 모델 컴파일링\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "# callback (ReduceLROnPlateau) 선언\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 모델 훈련과 동시에 train, validation 데이터에 대한 정확도와 loss 이력 저장\n",
    "history = model.fit(x=train_images, \n",
    "                    y=train_oh_targets, \n",
    "                    batch_size=64, \n",
    "                    epochs=20, \n",
    "                    validation_data=(validation_images, validation_oh_targets), callbacks=[rlr_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb1873-ff6a-41c7-97c6-a0ff168ae42a",
   "metadata": {},
   "source": [
    "#### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c207ae-1375-4793-92cc-1446adeb455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.7299 - loss: 0.7767 - val_acc: 0.8461 - val_loss: 0.4236\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8518 - loss: 0.4123 - val_acc: 0.8565 - val_loss: 0.3979\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8665 - loss: 0.3713 - val_acc: 0.8645 - val_loss: 0.3662\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8756 - loss: 0.3377 - val_acc: 0.8753 - val_loss: 0.3492\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8851 - loss: 0.3127 - val_acc: 0.8648 - val_loss: 0.3655\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8897 - loss: 0.2963 - val_acc: 0.8772 - val_loss: 0.3384\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8923 - loss: 0.2879 - val_acc: 0.8789 - val_loss: 0.3240\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8986 - loss: 0.2761 - val_acc: 0.8808 - val_loss: 0.3183\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9030 - loss: 0.2623 - val_acc: 0.8800 - val_loss: 0.3222\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9035 - loss: 0.2577 - val_acc: 0.8857 - val_loss: 0.3164\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9089 - loss: 0.2463 - val_acc: 0.8701 - val_loss: 0.3654\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9103 - loss: 0.2399 - val_acc: 0.8814 - val_loss: 0.3313\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9136 - loss: 0.2338 - val_acc: 0.8847 - val_loss: 0.3328\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 모델 컴파일링\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "# callback (EarlyStopping) 선언\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 모델 훈련과 동시에 train, validation 데이터에 대한 정확도와 loss 이력 저장\n",
    "history = model.fit(x=train_images, \n",
    "                    y=train_oh_targets, \n",
    "                    batch_size=64, \n",
    "                    epochs=20, \n",
    "                    validation_data=(validation_images, validation_oh_targets), callbacks=[ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e102371-1377-4910-bd3c-d448a62e7b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bb433-8925-4e1e-af38-3bb738ba5afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee81101-1778-425a-bbb6-84b1adfbc06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedaf7b-965f-4aa6-855b-f33d18e2198c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbe77e-29af-472c-b85f-492215c7737a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
